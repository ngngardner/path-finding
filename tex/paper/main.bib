@article{yuan_novel_2019,
  title    = {A {Novel} {GRU}-{RNN} {Network} {Model} for {Dynamic} {Path} {Planning} of {Mobile} {Robot}},
  volume   = {7},
  issn     = {2169-3536},
  doi      = {10.1109/ACCESS.2019.2894626},
  abstract = {A dynamic path planning method based on a gated recurrent unit-recurrent neural network model is proposed for the problem of path planning of a mobile robot in an unknown space. A deep neural network with sensor input is used to generate a new control strategy output to the physical model to control the movement of the robot and thus achieve collision avoidance behavior. Inputs and tags are derived from sample sets generated by an improved artificial potential field and an improved ant colony optimization algorithm. In order to make the ant colony algorithm converge quickly, the pheromone trail and the state transition probability are improved. The field function of the artificial potential field method is modified. Using the end-to-end network model to learn the mapping between input and output in the sample data, the direction and speed of the mobile robot are obtained. The simulation experiments and realistic simulations show that the network model can plan a reasonable path in an unknown environment. Compared with other traditional path planning algorithms, the proposed method is more robust than the traditional path planning algorithms to differences in the robot structure.},
  journal  = {IEEE Access},
  author   = {Yuan, Jianya and Wang, Hongjian and Lin, Changjian and Liu, Dawei and Yu, Dan},
  year     = {2019},
  keywords = {Collision avoidance, Mobile robots, Robot sensing systems, Path planning, Robot kinematics, Heuristic algorithms, Mobile robot, gated recurrent unit-recurrent neural network, dynamic path planning, ant colony optimization, artificial potential field},
  pages    = {15140--15151}
}

@inproceedings{nair_robotic_2020,
  title     = {Robotic {Path} {Planning} {Using} {Recurrent} {Neural} {Networks}},
  doi       = {10.1109/ICCCNT49239.2020.9225479},
  abstract  = {Every autonomous vehicle or any other application which requires reaching a destination, path planning is an important task. Once a destination is determined there can be various ways to reach there, but optimum use of resources to reach there is important. Hence path planning is an important part of navigation. There have been many developments over the years to realize the best path using different techniques. Recent developments on Neural Networks made it an important research topic for path planning. Many classifiers, learning algorithms have been tested and experimented for path planning. Deep Neural Networks have recently developed and seems to given better results compared to other machine learning techniques and there is larger space for research as well. CNN (Convolutional Neural Network) under the deep neural networks are observed and regarded to be usually applicable for path planning. But in this study Recurrent Neural Network technique (RNN) is being proposed. This is because RNN uses past inputs for present output which makes it applicable for temporal data. Hence in dynamic obstacle avoidance environment this method can help to predict path quicker than CNN. LSTM (Long Short Term Memory) is the algorithm that is mainly focused in this paper. LSTM is an extended version of RNN and this makes it suitable for congested environment. In this paper LSTM is applied for path planning in environment with obstacles and compared with the results of highly popular A* algorithm.},
  booktitle = {2020 11th {International} {Conference} on {Computing}, {Communication} and {Networking} {Technologies} ({ICCCNT})},
  author    = {Nair, Ramya S. and Supriya, P.},
  month     = jul,
  year      = {2020},
  keywords  = {Path planning, Robots, Navigation, Planning, Heuristic algorithms, Recurrent neural networks, Ions, Path planning, obstacles, RNN, LSTM, A*},
  pages     = {1--5}
}

@inproceedings{janet_essential_1995,
  author    = {Janet, J.A. and Luo, R.C. and Kay, M.G.},
  booktitle = {Proceedings of 1995 IEEE International Conference on Robotics and Automation},
  title     = {The essential visibility graph: an approach to global motion planning for autonomous mobile robots},
  year      = {1995},
  volume    = {2},
  number    = {},
  pages     = {1958-1963 vol.2},
  doi       = {10.1109/ROBOT.1995.526023}
}


@article{hanson_performance_2018,
  title    = {Performance {Characteristics} of {Robotic} {Mobile} {Fulfilment} {Systems} in {Order} {Picking} {Applications}},
  volume   = {51},
  issn     = {24058963},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S2405896318314149},
  doi      = {10.1016/j.ifacol.2018.08.290},
  language = {en},
  number   = {11},
  urldate  = {2022-04-22},
  journal  = {IFAC-PapersOnLine},
  author   = {Hanson, Robin and Medbo, Lars and Johansson, Mats I.},
  year     = {2018},
  pages    = {1493--1498}
}

@article{abd_algfoor_comprehensive_2015,
  title    = {A {Comprehensive} {Study} on {Pathfinding} {Techniques} for {Robotics} and {Video} {Games}},
  volume   = {2015},
  issn     = {1687-7047, 1687-7055},
  url      = {http://www.hindawi.com/journals/ijcgt/2015/736138/},
  doi      = {10.1155/2015/736138},
  abstract = {This survey provides an overview of popular pathfinding algorithms and techniques based on graph generation problems. We focus on recent developments and improvements in existing techniques and examine their impact on robotics and the video games industry. We have categorized pathfinding algorithms based on a 2D/3D environment search. The aim of this paper is to provide researchers with a thorough background on the progress made in the last 10 years in this field, summarize the principal techniques, and describe their results. We also give our expectations for future trends in this field and discuss the possibility of using pathfinding techniques in more extensive areas.},
  language = {en},
  urldate  = {2022-04-22},
  journal  = {International Journal of Computer Games Technology},
  author   = {Abd Algfoor, Zeyad and Sunar, Mohd Shahrizal and Kolivand, Hoshang},
  year     = {2015},
  pages    = {1--11}
}

@article{surynek_compilation-based_2021,
  title      = {Compilation-based {Solvers} for {Multi}-{Agent} {Path} {Finding}: a {Survey}, {Discussion}, and {Future} {Opportunities}},
  shorttitle = {Compilation-based {Solvers} for {Multi}-{Agent} {Path} {Finding}},
  url        = {http://arxiv.org/abs/2104.11809},
  abstract   = {Multi-agent path finding (MAPF) attracts considerable attention in artificial intelligence community as well as in robotics, and other fields such as warehouse logistics. The task in the standard MAPF is to find paths through which agents can navigate from their starting positions to specified individual goal positions. The combination of two additional requirements makes the problem computationally challenging: (i) agents must not collide with each other and (ii) the paths must be optimal with respect to some objective. Two major approaches to optimal MAPF solving include (1) dedicated search-based methods, which solve MAPF directly, and (2) compilation-based methods that reduce a MAPF instance to an instance in a different well established formalism, for which an efficient solver exists. The compilation-based MAPF solving can benefit from advancements accumulated during the development of the target solver often decades long. We summarize and compare contemporary compilation-based solvers for MAPF using formalisms like ASP, MIP, and SAT. We show the lessons learned from past developments and current trends in the topic and discuss its wider impact.},
  urldate    = {2022-04-22},
  journal    = {arXiv:2104.11809 [cs]},
  author     = {Surynek, Pavel},
  month      = apr,
  year       = {2021},
  note       = {arXiv: 2104.11809},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems}
}

@article{kyaw_coverage_2020,
  title    = {Coverage {Path} {Planning} for {Decomposition} {Reconfigurable} {Grid}-{Maps} {Using} {Deep} {Reinforcement} {Learning} {Based} {Travelling} {Salesman} {Problem}},
  volume   = {8},
  issn     = {2169-3536},
  doi      = {10.1109/ACCESS.2020.3045027},
  abstract = {Optimizing the coverage path planning (CPP) in robotics has become essential to accomplish efficient coverage applications. This work presents a novel approach to solve the CPP problem in large complex environments based on the Travelling Salesman Problem (TSP) and Deep Reinforcement Learning (DRL) leveraging the grid-based maps. The proposed algorithm applies the cellular decomposition methods to decompose the environment and generate the coverage path by recursively solving each decomposed cell formulated as TSP. A solution to TSP is determined by training Recurrent Neural Network (RNN) with Long Short Term Memory (LSTM) layers using Reinforcement Learning (RL). We validated the proposed method by systematically benchmarked with other conventional methods in terms of path length, execution time, and overlapping rate under four different map layouts with various obstacle density. The results depict that the proposed method outperforms all considered parameters than the conventional schemes. Moreover, simulation experiments demonstrate that the proposed approach is scalable to the larger grid-maps and guarantees complete coverage with efficiently generated coverage paths.},
  journal  = {IEEE Access},
  author   = {Kyaw, Phone Thiha and Paing, Aung and Thu, Theint Theint and Mohan, Rajesh Elara and Vu Le, Anh and Veerajagadheswar, Prabakaran},
  year     = {2020},
  keywords = {Robots, Reinforcement learning, Path planning, Recurrent neural networks, Training, Shape, Sensors, Coverage path planning, cellular reconfigurable decomposition, deep reinforcement learning, recurrent neural network, travelling salesman problem},
  pages    = {225945--225956}
}

@article{guo_fusion_2021,
  title    = {A {Fusion} {Method} of {Local} {Path} {Planning} for {Mobile} {Robots} {Based} on {LSTM} {Neural} {Network} and {Reinforcement} {Learning}},
  volume   = {2021},
  issn     = {1563-5147, 1024-123X},
  url      = {https://www.hindawi.com/journals/mpe/2021/5524232/},
  doi      = {10.1155/2021/5524232},
  abstract = {Due to the limitation of mobile robots’ understanding of the environment in local path planning tasks, the problems of local deadlock and path redundancy during planning exist in unknown and complex environments. In this paper, a novel algorithm based on the combination of a long short-term memory (LSTM) neural network, fuzzy logic control, and reinforcement learning is proposed, and uses the advantages of each algorithm to overcome the other’s shortcomings. First, a neural network model including LSTM units is designed for local path planning. Second, a low-dimensional input fuzzy logic control (FL) algorithm is used to collect training data, and a network model (LSTM\_FT) is pretrained by transferring the learned method to learn the basic ability. Then, reinforcement learning is combined to learn new rules from the environments autonomously to better suit different scenarios. Finally, the fusion algorithm LSTM\_FTR is simulated in static and dynamic environments, and compared to FL and LSTM\_FT algorithms, respectively. Numerical simulations show that, compared to FL, LSTM\_FTR can significantly improve decision-making efficiency, improve the success rate of path planning, and optimize the path length. Compared to the LSTM\_FT, LSTM\_FTR can improve the success rate and learn new rules.},
  language = {en},
  urldate  = {2022-04-22},
  journal  = {Mathematical Problems in Engineering},
  author   = {Guo, Na and Li, Caihong and Gao, Tengteng and Liu, Guoming and Li, Yongdi and Wang, Di},
  editor   = {Hariri-Ardebili, Mohammad Amin},
  month    = jun,
  year     = {2021},
  pages    = {1--21}
}

@inproceedings{shen_path-following_2016,
  title     = {Path-following control of underactuated ships using actor-critic reinforcement learning with {MLP} neural networks},
  doi       = {10.1109/ICIST.2016.7483431},
  abstract  = {In this paper, the critic-actor reinforcement learning method is proposed to solve the path-following problem of underactuated ships. In order to facilitate the design of the reward function, ship path-following control is transformed into calculating a reference course to reduce cross tracking error by the methods of coordinate compression and drift angle compensation. For the purpose of overcoming the curse of dimensionality caused by the continuous state spaces and action spaces of ships, the multilayer perception (MLP) neural networks are used to approximate the value function. Finally, the simulation with the environmental disturbances including wind, wave and ocean current, is presented to demonstrate the effectiveness and the performance of the proposed scheme.},
  booktitle = {2016 {Sixth} {International} {Conference} on {Information} {Science} and {Technology} ({ICIST})},
  author    = {Shen, Haiqing and Guo, Chen},
  month     = may,
  year      = {2016},
  keywords  = {underactuated ships, path-following control, reinforcement learning, actor-critic, neural networks},
  pages     = {317--321}
}

@article{shalev-shwartz_safe_2016,
  title    = {Safe, {Multi}-{Agent}, {Reinforcement} {Learning} for {Autonomous} {Driving}},
  url      = {http://arxiv.org/abs/1610.03295},
  abstract = {Autonomous driving is a multi-agent setting where the host vehicle must apply sophisticated negotiation skills with other road users when overtaking, giving way, merging, taking left and right turns and while pushing ahead in unstructured urban roadways. Since there are many possible scenarios, manually tackling all possible cases will likely yield a too simplistic policy. Moreover, one must balance between unexpected behavior of other drivers/pedestrians and at the same time not to be too defensive so that normal traffic flow is maintained. In this paper we apply deep reinforcement learning to the problem of forming long term driving strategies. We note that there are two major challenges that make autonomous driving different from other robotic tasks. First, is the necessity for ensuring functional safety - something that machine learning has difficulty with given that performance is optimized at the level of an expectation over many instances. Second, the Markov Decision Process model often used in robotics is problematic in our case because of unpredictable behavior of other agents in this multi-agent scenario. We make three contributions in our work. First, we show how policy gradient iterations can be used without Markovian assumptions. Second, we decompose the problem into a composition of a Policy for Desires (which is to be learned) and trajectory planning with hard constraints (which is not learned). The goal of Desires is to enable comfort of driving, while hard constraints guarantees the safety of driving. Third, we introduce a hierarchical temporal abstraction we call an "Option Graph" with a gating mechanism that significantly reduces the effective horizon and thereby reducing the variance of the gradient estimation even further.},
  urldate  = {2022-04-22},
  journal  = {arXiv:1610.03295 [cs, stat]},
  author   = {Shalev-Shwartz, Shai and Shammah, Shaked and Shashua, Amnon},
  month    = oct,
  year     = {2016},
  note     = {arXiv: 1610.03295},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{carrio_review_2017,
  title    = {A {Review} of {Deep} {Learning} {Methods} and {Applications} for {Unmanned} {Aerial} {Vehicles}},
  volume   = {2017},
  issn     = {1687-725X, 1687-7268},
  url      = {https://www.hindawi.com/journals/js/2017/3296874/},
  doi      = {10.1155/2017/3296874},
  abstract = {Deep learning is recently showing outstanding results for solving a wide variety of robotic tasks in the areas of perception, planning, localization, and control. Its excellent capabilities for learning representations from the complex data acquired in real environments make it extremely suitable for many kinds of autonomous robotic applications. In parallel, Unmanned Aerial Vehicles (UAVs) are currently being extensively applied for several types of civilian tasks in applications going from security, surveillance, and disaster rescue to parcel delivery or warehouse management. In this paper, a thorough review has been performed on recent reported uses and applications of deep learning for UAVs, including the most relevant developments as well as their performances and limitations. In addition, a detailed explanation of the main deep learning techniques is provided. We conclude with a description of the main challenges for the application of deep learning for UAV-based solutions.},
  language = {en},
  urldate  = {2022-04-22},
  journal  = {Journal of Sensors},
  author   = {Carrio, Adrian and Sampedro, Carlos and Rodriguez-Ramos, Alejandro and Campoy, Pascual},
  year     = {2017},
  pages    = {1--13}
}

@inproceedings{inoue_robot_2019,
  address   = {Singapore},
  title     = {Robot {Path} {Planning} by {LSTM} {Network} {Under} {Changing} {Environment}},
  isbn      = {9789811303418},
  doi       = {10.1007/978-981-13-0341-8_29},
  abstract  = {Path planning is an important function for executing autonomous moving robots, and many path planning methods that satisfy various constraints, such as avoiding obstacles and energy efficiency, have been proposed. However, these conventional methods have several difficulties for apply to the actual applications, such as the instability, low reproducibility, huge training data set required. Therefore, we propose a novel robot path planning method that combines the rapidly exploring random tree (RRT) and long short-term memory (LSTM) network. In this method, numerous and good paths are generated in the robot configuration space by the RRT method, a convolutional autoencoder and LSTM combination network is trained by them. The proposed method overcomes the difficulty of general methods with neural networks, i.e., “the acquisition of a large amount of training data.” Moreover, the difficulty of general random based methods, i.e., “the reproducible path generation” is resolved with high-speed.},
  language  = {en},
  booktitle = {Advances in {Computer} {Communication} and {Computational} {Sciences}},
  publisher = {Springer},
  author    = {Inoue, Masaya and Yamashita, Takahiro and Nishida, Takeshi},
  editor    = {Bhatia, Sanjiv K. and Tiwari, Shailesh and Mishra, Krishn K. and Trivedi, Munesh C.},
  year      = {2019},
  pages     = {317--329}
}

@inproceedings{khan_using_2017,
  title     = {Using recurrent neural networks ({RNNs}) as planners for bio-inspired robotic motion},
  doi       = {10.1109/CCTA.2017.8062594},
  abstract  = {In this paper, we propose using Long Short Term Memory Networks (LSTM) to serve as planners for bio-inspired robotic motion. LSTMs can learn long or short term correlations of sequential data. Using LSTM networks, we implement a motion planner using simulated fish trajectories. The motion or path planning unit can then be implemented on robots such that they can operate autonomously without knowing their absolute position in a global frame. Simulation results show that the planned path demonstrates characteristics that are similar to simulated fish trajectories. This work may lead to learning animal behavior and then formulating bio-inspired path planners for robots to operate in unknown environments.},
  booktitle = {2017 {IEEE} {Conference} on {Control} {Technology} and {Applications} ({CCTA})},
  author    = {Khan, Ayesha and Zhang, Fumin},
  month     = aug,
  year      = {2017},
  keywords  = {Trajectory, Robots, Hidden Markov models, Logic gates, Animals, Recurrent neural networks},
  pages     = {1025--1030}
}

@article{liu_improved_2021,
  title    = {Improved {Deep} {Reinforcement} {Learning} with {Expert} {Demonstrations} for {Urban} {Autonomous} {Driving}},
  url      = {http://arxiv.org/abs/2102.09243},
  abstract = {Currently, urban autonomous driving remains challenging because of the complexity of the driving environment. Learning-based approaches, such as reinforcement learning (RL) and imitation learning (IL), have indicated superiority over rule-based approaches, showing great potential to make decisions intelligently, but they still do not work well in urban driving situations. To better tackle this problem, this paper proposes a novel learning-based method that combines deep reinforcement learning with expert demonstrations, focusing on longitudinal motion control in autonomous driving. Our proposed method employs the soft actor-critic structure and modifies the learning process of the policy network to incorporate both the goals of maximizing reward and imitating the expert. Moreover, an adaptive prioritized experience replay is designed to sample experience from both the agent's self-exploration and expert demonstration, in order to improve the sample efficiency. The proposed method is validated in a simulated urban roundabout scenario and compared with various prevailing RL and IL baseline approaches. The results manifest that the proposed method has a faster training speed, as well as better performance in navigating safely and time-efficiently. The ablation study reveals that the prioritized replay and expert demonstration filter play important roles in our proposed method.},
  urldate  = {2022-04-22},
  journal  = {arXiv:2102.09243 [cs]},
  author   = {Liu, Haochen and Huang, Zhiyu and Lv, Chen},
  month    = jun,
  year     = {2021},
  note     = {arXiv: 2102.09243},
  keywords = {Computer Science - Robotics}
}

@article{hester_deep_2017,
  title    = {Deep {Q}-learning from {Demonstrations}},
  url      = {http://arxiv.org/abs/1704.03732},
  abstract = {Deep reinforcement learning (RL) has achieved several high profile successes in difficult decision-making problems. However, these algorithms typically require a huge amount of data before they reach reasonable performance. In fact, their performance during learning can be extremely poor. This may be acceptable for a simulator, but it severely limits the applicability of deep RL to many real-world tasks, where the agent must learn in the real environment. In this paper we study a setting where the agent may access data from previous control of the system. We present an algorithm, Deep Q-learning from Demonstrations (DQfD), that leverages small sets of demonstration data to massively accelerate the learning process even from relatively small amounts of demonstration data and is able to automatically assess the necessary ratio of demonstration data while learning thanks to a prioritized replay mechanism. DQfD works by combining temporal difference updates with supervised classification of the demonstrator's actions. We show that DQfD has better initial performance than Prioritized Dueling Double Deep Q-Networks (PDD DQN) as it starts with better scores on the first million steps on 41 of 42 games and on average it takes PDD DQN 83 million steps to catch up to DQfD's performance. DQfD learns to out-perform the best demonstration given in 14 of 42 games. In addition, DQfD leverages human demonstrations to achieve state-of-the-art results for 11 games. Finally, we show that DQfD performs better than three related algorithms for incorporating demonstration data into DQN.},
  urldate  = {2022-04-22},
  journal  = {arXiv:1704.03732 [cs]},
  author   = {Hester, Todd and Vecerik, Matej and Pietquin, Olivier and Lanctot, Marc and Schaul, Tom and Piot, Bilal and Horgan, Dan and Quan, John and Sendonaris, Andrew and Dulac-Arnold, Gabriel and Osband, Ian and Agapiou, John and Leibo, Joel Z. and Gruslys, Audrunas},
  month    = nov,
  year     = {2017},
  note     = {arXiv: 1704.03732},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning}
}
